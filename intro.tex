\section{Introduction}
\label{intro}
The act of fuel cycle benchmarking has long faced methodological issues 
as per what metrics to compare, how to compare them, and at what point in the
fuel cycle they should be compared 
\cite{wilson2011comparing,guerin2009benchmark,piet2011assessment}. 
The benchmarking mechanism described 
here couples Gaussian process models (GP) \cite{rasmussen2006gaussian} to 
dynamic time warping (DTW) \cite{muller}. Together these address how to 
generate figures-of-merit (FOM) for common nuclear fuel cycle benchmarking
tasks. 

Confusion in this area is partly because such activities 
are not in fact `benchmarking' in the strictest validation sense. Most fuel
cycle benchmarks are more correctly called code-to-code comparisons, or 
inter-code comparisons, as they compare simulator results. Importantly, 
there is an absence of true experimental data for a benchmark or simulator
to validate against. Moreover, the number of 
real world, industrial scale nuclear fuel cycles that have historically been 
deployed is not sufficient for statistical accuracy. Even the canonical 
once-through fuel cycle scenario has only been deployed a handful of times.
For other advanced fuel cycles, industrial scale data is even more scarce. 
Since fuel cycle simulation is thus effectively impossible to validate, 
non-judgmental methods of benchmarking must be considered. 
Define a \emph{non-judgmental method} as one such that the 
results of any given simulator is evaluated in reference to how 
it performs against other simulators in such a way that acknowledges that 
any and all simulators may demonstrate incorrect behavior. No simulator
by fiat produces the `true' or reference answer because all simulators 
cannot be validated. Picking a reference solution biases further analysis 
towards that solution and, implicitly or explicitly, determines that the
chosen simulator is the most valid one for purposes of the benchmark. The 
aim of a non-judgmental benchmark is to avoid all such biases.

The other major conceptual issue with fuel cycle benchmarking is that there 
is no agreed upon mechanism for establishing a figure-of-merit
that is uniform across all fuel cycle metrics of interest. For example, 
repository heat load may be examined only at the end of the of the simulation,
separated plutonium may be used as a FOM wherever it peaks, and natural uranium 
mined might be of concern only after 100 years from the start of the 
simulation if resource scarcity is anticipated. Comparing at a specific point 
in time fails to take into account the behavior of that metric over time and 
can skew decision making. Additionally, the appropriate 
time of comparison depends on the metric itself, and is thus not necessarily 
the same appropriate time for another metric. This is a necessary 
side effect of picking a single point in time.
Furthermore, such FOMs are not useful for indicating why simulations differ, 
only that they do. Moreover, if such FOMs match, this does not indicate
that the simulators actually agree; their behavior 
could be radically different at every other simulation time and converge
only where the metric is evaluated.  The one caveat 
here is that 
equilibrium and quasi-static fuel cycle simulators are sometimes able to 
ignore these issues because all points in time are treated equally.

Some dynamic FOMs do exist, such as aggregating of the high- and low-level
waste generation per year metrics seen in the recent 
``Nuclear Fuel Cycle Evaluation and Screening - Final Report'' 
\cite{wigeland2014nuclear}. However, these typically require that the metric
data be too well-behaved for generic comparison purposes. Consider  
total power produced [GWe] as the metric and a FOM for this metric as 
the sum over time of the relative error 
between the total power of a single simulator and the root-mean squared 
total power of all the simulators together. This FOM has been suggested as
a candidate for the upcoming ``Open Fuel Cycle Benchmark'' 
\cite{mouginot2015ofcb}. However, such an FOM cannot be calculated 
if the total power
time series have different lengths. Such differences could arise because 
of different time steps (1 month versus 1 year) or because of different 
simulation durations. Furthermore, suppose that a benchmark is posed as 
"until transition" in a transition scenario. It would defeat the purpose of 
the benchmark to force different simulators to have the same 
time-to-transition if they nominally would calculate distinct transition 
times. Therefore, a robust FOM should not impose the constraint of a uniform time grid.
 
The mechanisms used for benchmarking that have been discussed so far typically
incorporate neither uncertainty in the input parameters nor 
modeling uncertainty from the simulator.
This is likely because most simulators do not compute uncertainty from their
inputs nor do they report uncertainties associated with the algorithms that 
they implement. 
Instead they rely on perturbation studies or stochastic wrappers around 
the simulator. Furthermore, many metrics may rely on external nuclear 
data (such as half-lives and cross-sections), which have corresponding 
uncertainties that should effect benchmarking comparisons. 
However, even if such error bars were available for
every point in a time series metric, the traditional benchmark FOM 
calculations would ignore them. The simple aggregation of metrics from 
\cite{wigeland2014nuclear} does not produce uncertainty on the resultant
total and neither does the root-mean squared difference FOM 
\cite{mouginot2015ofcb}. Only the nominal value is used in these situations.

The purpose of this paper is to describe a methodology that generates FOMs
for all potential fuel cycle metrics in the same way such that the FOMs 
incorporate any provided uncertainty. 
Before going further, it is important to note that 
most fuel cycle 
metrics are time series and can be derived from the mass balances (or 
mass flows) \cite{wilson2011comparing,guerin2009benchmark,piet2011assessment,wigeland2014nuclear}. 
Additionally, many metrics have an associated total metric that can be 
computed from the linear combination of all of its constituent features. 
For example, total mass flows per time step are the sum of the mass flow 
of each nuclide per time step
and total power generated is the sum of the power from each reactor type, 
such as light water reactors (LWR) and fast reactors (FR). These attributes 
are common to the overwhelming majority of fuel cycle metrics and so FOMs
may take advantage of such structures.

Gaussian process models are proposed here as a method to incorporate 
metric uncertainties and make the analysis non-judgmental with respect to 
any particular
simulator. Roughly speaking, a Gaussian process regression is a 
statistical technique
that models the relationship between independent and dependent parameters
by fitting the covariance to a nominal functional form, or kernel.
The kernel may have as many or as few fit parameters (also called 
\emph{hyperparameters}) as desired. One often used kernel is the squared 
exponential, or Gaussian distribution \cite{rasmussen2006gaussian,hodlr}. 
Linear kernels and Gaussian noise kernels are also frequently used 
alternatives (see \cite{rasmussen2006gaussian,hodlr} again). 

Using a Gaussian process is desirable because the model can be generated
from as many simulators as available and it will treat the results of
each simulators in the same manner. Unlike a relative error analysis, no 
simulator needs to be taken as the fiducial case; the Gaussian process model 
itself becomes the target to compare against. 
Moreover, the covariances do not need to be known to perform the benchmark
since they are estimated by the Gaussian process. Furthermore, once the 
hyperparameters are known, this can be used as a representative model for 
any desired time grid. Additionally, the incorporation of the uncertainties
in a Gaussian process are known to be more accurate (closer means) than 
assuming uncorrelated uncertainties \cite{rasmussen1996evaluation,ko2009gp}.
The trade off is that the model is less precise (higher standard deviations) 
than the uncorrelated case \cite{hodlr}. Such a trade off is likely 
desirable because no simulator 
is necessarily more valid than any other simulator. For example, 
in an inter-code comparison, an outlier simulator may be the 
closest to the true solution, perhaps because it is higher fidelity than 
the others. Thus it becomes important to quantify outliers rather than 
discard them.

However, a Gaussian process model of a metric for a set of simulators 
does not directly present itself as a FOM for that metric. Time series from 
all simulators go into a Gaussian process and a representative model time 
series comes out. The Gaussian process handles making the benchmark
non-judgmental and incorporating uncertainties. However, Gaussian processes 
do not 
perform a reduction in dimensionality from time series into to a simple 
scalar. The dynamic time warping 
technique is proposed as a method for deriving FOMs from the models that 
does reduce the dimensionality to an easily comparable single number.

Dynamic time warping computes the minimal distance and path that it would 
take to convert one time series curve into another. This procedure is highly 
leveraged in audio processing systems, and especially in speech recognition
\cite{myers1980performance,muda2010voice}.
This is due to the DTW advantage that the two time series that are being 
compared need not have the same time grid. In other words, it does not matter 
if one time series is longer than 
the other or if they have different sampling frequencies. The DTW distance
instead compares the shape of the curves themselves. This is beneficial as
a FOM derived from the DTW distance can thus represent the structural 
similarity of the underlying time series.
 

There is nothing about the dynamic time warping algorithm that is specific 
to speech recognition and the method
may be used for any time series analysis. For nuclear fuel cycle benchmarking,
the DTW distance can be used to compare the metric from each simulator to its
Gaussian process model. Using this as an FOM has the advantage of 
incorporating information from the whole time series, rather than just a 
specific point in the cycle.

Many benchmarking studies also wish to create a rank ordering of parameter
importance over all simulators. Examples of such benchmark questions include, 
``In a transition scenario,
which reactor contributes the most to the total generated power?'' and 
``Which
nuclides are most important to the repository heat load?'' DTW distances 
of Gaussian process models of the constient parameters (e.g. the power from
each reactor type) to a Gaussian process model of the total (e.g. total 
generated power) can be used as a FOM itself or to derive other FOMs. 
This paper proposes a novel contribution metric
that is taken to be a normalized version of the DTW 
distance for such rank ordering activities.

Additionally note that a fuel cycle benchmark is distinct from a 
fuel cycle evaluation, such as the one presented in \cite{wigeland2014nuclear}.
In a benchmark, a single scenario (such the once-through or a transition 
scenario) is analyzed with respect to the results coming from multiple 
simulators. In an evaluation, on the other hand, the focus of the analysis
is on the performance differences between many scenarios. If many simulators
are used for each scenario in an evaluation, an inter-code comparison may 
be a step within the evaluation. The methodology described in this paper
applies to such a benchmarking step. However, to enable comparisons
between different scenarios that are the core part of a fuel cycle evaluation,
this method requires modification. Such additions to the methodology 
are not present in this paper, whose focus is benchmarking, but will
be forthcoming in future work. 

The remainder of this paper takes a narrative approach that walks through 
a fictitious example benchmarking activity. Example generated power data 
(in gigawatts electric [GWe]) 
from DYMOND \cite{yacout2005modeling,feng2015dymond} and Cyclus 
\cite{DBLP:journals/corr/HuffGCFMOSSW15,cyclus_v1_0} are used. 
The underlying fuel cycle being modeled is an
LWR to FR transition scenario that occurs over 200 years, starting from 2010.  
This data should be regarded as for demonstration purposes only. No deep
fuel cycle truths should be directly inferred and the data itself should be 
considered preliminary. However, 
using results from actual fuel cycle simulators shows how
non-judgmental benchmarking works in practice. Arbitrarily constructed 
or randomly generated data could 
have been used instead, but this demonstration of the method is more 
motivating to future benchmarking activities.

In \S \ref{setup}, the benchmark problem is set up, mathematical notation is 
introduced,
and the raw data from the simulators are presented. In \S \ref{gp}, Gaussian 
process 
modeling is introduced. In \S \ref{dtw}, the dynamic time warping concept is
discussed. In \S \ref{contribution}, the novel contribution metric is 
derived.
\S \ref{filtering} warns against standard time series filtering
techniques. And finally, \S \ref{conclusion} contains concluding remarks
and ideas for future work in fuel cycle benchmarking and for 
using the mechanisms presented here.
