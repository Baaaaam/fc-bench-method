\section{Introduction}
\label{intro}
The act of fuel cycle benchmarking has long faced methodological issues 
on what metrics to compare, how to compare them, and at what point in the
fuel cycle they should be compared 
\cite{wilson2011comparing,guerin2009benchmark,piet2011assessment}. 
The benchmarking mechanism described 
here couples Gaussian process models (GP) \cite{rasmussen2006gaussian} to 
dynamic time warping (DTW) \cite{muller}. Together these address how to 
generate figures-of-merit (FOM) for common nuclear fuel cycle benchmarking
tasks. 

Confusion in this area is partly because such activities 
are not in fact `benchmarking' in the strictest validation sense. Most fuel
cycle benchmarks are more correctly called code-to-code comparisons, or 
inter-code comparisons, as they compare simulator results. Importantly, 
there is an absence of true experimental data for a benchmark or simulator
to validate against. Moreover, the number of 
real world, industrial scale nuclear fuel cycles that have historically been 
deployed is not sufficient for statistical accuracy. Even the canonical 
Once Through fuel cycle scenario has only been deployed a handful of times.
For other advanced fuel cycles, industrial scale data is even more stark. 
Since fuel cycle simulation is thus effectively impossible to validate, 
non-judgmental methods of benchmarking must be considered. The 
results of any given simulator should be evaluated in reference to how 
it performs against other simulators in such a way that acknowledges that 
any and all simulators may demonstrate incorrect behavior. No simulator
by fiat produces the `true' or reference answer.

The other major conceptual issue with fuel cycle benchmarking is that there 
is no agreed upon mechanism for establishing a figure-of-merit for 
that is uniform across all fuel cycle metrics of interest. For example, 
repository heat load may be examined only at the end of the of the simulation,
separated plutonium may be used wherever it peaks, and natural uranium 
mined might be of concern only after 100 years from the start of the 
simulation. Comparing at a specific point 
in time fails to take into account the behavior of that metric over time and 
can skew decision making. Additionally, the 
time of comparison varies based on the metric itself. This is a necessary 
side effect of picking a single point in time.
Furthermore, such FOMs are not useful for indicating why simulations differ, 
only that they do. Moreover, if such FOMs match, this does not indicate
that the simulators actually agree. Their behavior 
could be radically different at every other simulation time and converge
only where the metric is evaluated.  The one caveat 
here is that 
equilibrium and quasi-static fuel cycle simulators are sometimes able to 
ignore these issues because all points in time are treated equally.

Some dynamic FOMs do exist. However, these typically require that the metric
data be too well-behaved for comparison purposes. Consider the case of total
power produced [GWe]. A FOM could be the sum over time of the relative error 
between the total power of a single simulator and the root-mean squared total power
of all the simulators together. However, such an FOM fails if the total power
time series have different lengths. Such differences could arise because 
of different time steps (1 month versus 1 year) or because of different 
simulation durations. Suppose that a comparison is posed as "until transition"
in a transition scenario, it could defeat the purpose of the benchmark to
force different simulators to have the same time-to-transition if they 
nominally would have had distinct. 
 
The mechanisms used for benchmarking that have been discussed so far typically
do not incorporate modeling uncertainty coming from the simulator itself.
This is likely because most simulators do not compute uncertainty on their 
own. Instead they rely on perturbation studies or stochastic wrappers around 
the simulator. Furthermore, metrics may add their own uncertainty from the 
data that the bring in (half-lives, cross-sections, etc.) and the 
operations they perform. However, even if such error bars were available for
every point in a time series metric, the benchmark FOM calculation would 
ignore them.

The method described in this paper addresses all of the above issues that 
that a dynamic fuel cycle benchmark will be able to use on the 
metrics of interest. It is important to note, though, that most fuel cycle 
metrics are time series and can be derived from the mass balances. 
Furthermore, most metrics have an associated total metric that can be 
computed from the linear combination of all of its constituent features. 
For example, total mass flows are the sum of the mass flow of each nuclide
and total power generated is the sum of the power from each reactor type 
(such as LWRs and FRs).  These attributes are common to the overwhelming 
majority of fuel cycle metrics.

Gaussian process models are proposed here as a method to incorporate 
metric uncertainties and make the analysis non-judgmental to any particular 
simulator. Roughly speaking, a Gaussian process is a regression technique
that models the relationship between independent and dependent parameters
by fitting the covariance to a nominal functional form, or kernel.
The kernel may have as many or as few fit parameters (also called 
hyperparameters) as desired. One often used kernel is the squared 
exponential, or Gaussian distribution, though linear kernels and Gaussian
noise kernels are also frequently used. 

Using this mechanism is desirable because the model can be generated
from as many simulator results as possible and it will treat each simulators
results in the same manner. Unlike a relative error analysis, no simulator
needs to be taken as the base case. The Gaussian process model itself becomes
the target to compare against. 
Moreover, the covariances do not need to be known to perform the benchmark.
They are estimated by the Gaussian process. Furthermore, once the 
hyperparameters are known, this can be used as a representative model for 
any other time series. Additionally, the incorporation of the uncertainties
in a Gaussian process are known to be more accurate (closer means) than 
assuming uncorrelated uncertainties.  The trade-off here is that the 
model is less precise (higher standard deviations) than the uncorrelated 
case. Such a trade off is likely desirable because no simulator is 
necessarily more correct than any other simulator. These features are all 
necessary for inter-code comparisons where an outlier simulator may be the 
most correct (perhaps it is higher fidelity than the others). Thus it
becomes important to quantify outliers rather than discard them.

However, a Gaussian process model of a metric for a set of simulators 
does not directly present itself as a FOM for that metric. Time series go 
into a Gaussian process and time series come out. The dynamic time warping 
technique is proposed as a method for deriving FOMs from the models.  

Dynamic time warping computes the minimal distance and path that it would 
take to convert one time series curve into another. This procedure is highly 
leveraged in audio processing systems, and especially in speech recognition.
This is because the two time series that are being compared need not have
the same time basis.  It does not matter if one time series is longer than 
the other or if they have different sampling frequencies. The DTW distance
instead compares the shape the curves themselves. 

There is nothing about DTW that is specific to speech recognition. The method
may be used in any time series analysis. For nuclear fuel cycle benchmarking,
the DTW distance can be used to compare the metric from each simulator to 
Gaussian process model. Using this as an FOM has the advantage of 
incorporating information from the whole time series, rather than just a 
specific point in the cycle.

Many benchmarking studies also wish to create a rank ordering of parameter
importance over all simulators.  For example, ``In a transition scenario,
which reactor is most important to the total generated power?'' or ``Which
nuclides are most important to the repository heat load?'' DTW distances 
of gaussian process models of the constient parameters (eg the power from
each reactor type) to a Gaussian process model of the total (eg total 
generated power) can be used as a FOM or to derive other FOMs. This paper
proposes a novel contribution metric, that is a normalized version of the DTW 
distance for such rank ordering activities.

The remainder of this paper takes a narrative approach which walks through 
a ficticious example benchmarking activity using generated power [GWe] data 
from DYMOND and Cyclus. The underlying fuel cycle being modeled is an
LWR to FR transition scenario that occurs over the following 200 years.  
This data should be regarded as for demonstration purposes only. However, 
using results from actual fuel cycle simultors shows how the non-judgemental
benchmarking works in practice. Completely faked data could have been used 
instead, but this demonstartion is more believable.

In \S \ref{setup}, the problem is setup, mathematical notation is introduced, 
and the raw simulator data is presented. In \S \ref{gp}, Gaussian process 
modeling is discussed. In \S \ref{dtw}, the dynamic time warping concept is
shown. In \S \ref{contribution}, the novel contribution metric is introduced.
Then in \S \ref{filtering}, the merits of standard time series filtering
techniques gone over. Finally, \S \ref{conclusion} contains concluding remarks
and ideas for future work in this area.
