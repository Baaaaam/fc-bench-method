\section{Introduction}
\label{intro}
The act of fuel cycle benchmarking has long faced methodological issues 
as per what metrics to compare, how to compare them, and at what point in the
fuel cycle they should be compared 
\cite{wilson2011comparing,guerin2009benchmark,piet2011assessment}. 
The benchmarking mechanism described 
here couples Gaussian process models (GP) \cite{rasmussen2006gaussian} to 
dynamic time warping (DTW) \cite{muller}. Together these address how to 
generate figures-of-merit (FOM) for common nuclear fuel cycle benchmarking
tasks. 

Confusion in this area is partly because such activities 
are not in fact `benchmarking' in the strictest validation sense. Most fuel
cycle benchmarks are more correctly called code-to-code comparisons, or 
inter-code comparisons, as they compare simulator results. Importantly, 
there is an absence of true experimental data for a benchmark or simulator
to validate against. Moreover, the number of 
real world, industrial scale nuclear fuel cycles that have historically been 
deployed is not sufficient for statistical accuracy. Even the canonical 
Once Through fuel cycle scenario has only been deployed a handful of times.
For other advanced fuel cycles, industrial scale data is even more stark. 
Since fuel cycle simulation is thus effectively impossible to validate, 
non-judgmental methods of benchmarking must be considered. The 
results of any given simulator should be evaluated in reference to how 
it performs against other simulators in such a way that acknowledges that 
any and all simulators may demonstrate incorrect behavior. No simulator
by fiat produces the `true' or reference answer.

The other major conceptual issue with fuel cycle benchmarking is that there 
is no agreed upon mechanism for establishing a figure-of-merit for 
that is uniform across all fuel cycle metrics of interest. For example, 
repository heat load may be examined only at the end of the of the simulation,
separated plutonium may be used as a FOM wherever it peaks, and natural uranium 
mined might be of concern only after 100 years from the start of the 
simulation. Comparing at a specific point 
in time fails to take into account the behavior of that metric over time and 
can skew decision making. Additionally, the 
time of comparison varies based on the metric itself. This is a necessary 
side effect of picking a single point in time.
Furthermore, such FOMs are not useful for indicating why simulations differ, 
only that they do. Moreover, if such FOMs match, this does not indicate
that the simulators actually agree. Their behavior 
could be radically different at every other simulation time and converge
only where the metric is evaluated.  The one caveat 
here is that 
equilibrium and quasi-static fuel cycle simulators are sometimes able to 
ignore these issues because all points in time are treated equally.

Some dynamic FOMs do exist. However, these typically require that the metric
data be too well-behaved for generic comparison purposes. Consider the case 
of total power produced [GWe]. A FOM could be the sum over time of the relative error 
between the total power of a single simulator and the root-mean squared total power
of all the simulators together. However, such an FOM fails if the total power
time series have different lengths. Such differences could arise because 
of different time steps (1 month versus 1 year) or because of different 
simulation durations. Furthermore, suppose that a benchmark is posed as 
"until transition" in a transition scenario. It would defeat the purpose of 
the benchmark to force different simulators to have the same 
time-to-transition if they nominally would calculate distinct transition 
times. Therefore, a robust FOM should not impose the constraint of a uniform time grid.
 
The mechanisms used for benchmarking that have been discussed so far typically
do not incorporate modeling uncertainty coming from the simulator itself.
This is likely because most simulators do not compute uncertainty directly. 
Instead they rely on perturbation studies or stochastic wrappers around 
the simulator. Furthermore, metrics may add their own uncertainty from the 
data that they bring in (half-lives, cross-sections, etc.). 
However, even if such error bars were available for
every point in a time series metric, the traditional benchmark FOM 
calculations would ignore them.

The method described in this paper addresses all of the above issues. It 
creates FOMs that a dynamic fuel cycle benchmark will be able to use on any 
metrics of interest. Before going further, it is important to note that 
most fuel cycle 
metrics are time series and can be derived from the mass balances. 
Additionally, many metrics have an associated total metric that can be 
computed from the linear combination of all of its constituent features. 
For example, total mass flows are the sum of the mass flow of each nuclide
and total power generated is the sum of the power from each reactor type, 
such as light water reactors (LWR) and fast reactors (FR). These attributes 
are common to the overwhelming majority of fuel cycle metrics and so FOMs
may take advantage of such structures.

Gaussian process models are proposed here as a method to incorporate 
metric uncertainties and make the analysis non-judgmental with respect to 
any particular
simulator. Roughly speaking, a Gaussian process regression is a 
statistical technique
that models the relationship between independent and dependent parameters
by fitting the covariance to a nominal functional form, or kernel.
The kernel may have as many or as few fit parameters (also called 
\emph{hyperparameters}) as desired. One often used kernel is the squared 
exponential, or Gaussian distribution. Linear kernels and Gaussian
noise kernels are also frequently used alternatives. 

Using a Gaussian process is desirable because the model can be generated
from as many simulators as available and it will treat the results of
each simulators in the same manner. Unlike a relative error analysis, no 
simulator needs to be taken as the fiducial case. The Gaussian process model 
itself becomes the target to compare against. 
Moreover, the covariances do not need to be known to perform the benchmark.
They are estimated by the Gaussian process. Furthermore, once the 
hyperparameters are known, this can be used as a representative model for 
any desired time grid. Additionally, the incorporation of the uncertainties
in a Gaussian process are known to be more accurate (closer means) than 
assuming uncorrelated uncertainties.  The trade off is that the 
model is less precise (higher standard deviations) than the uncorrelated 
case \cite{hodlr}. Such a trade off is likely desirable because no simulator 
is necessarily more correct than any other simulator. For example, 
in an inter-code comparison, an outlier simulator may be the 
most correct, perhaps because it is higher fidelity than the others. Thus it
becomes important to quantify outliers rather than discard them.

However, a Gaussian process model of a metric for a set of simulators 
does not directly present itself as a FOM for that metric. Time series go 
into a Gaussian process and time series come out. The dynamic time warping 
technique is proposed as a method for deriving FOMs from the models.  

Dynamic time warping computes the minimal distance and path that it would 
take to convert one time series curve into another. This procedure is highly 
leveraged in audio processing systems, and especially in speech recognition
\cite{myers1980performance,muda2010voice}.
This is because the two time series that are being compared need not have
the same time basis.  It does not matter if one time series is longer than 
the other or if they have different sampling frequencies. The DTW distance
instead compares the shape the curves themselves. 

There is nothing about the dynamic time warping algorithm that is specific 
to speech recognition. The method
may be used in any time series analysis. For nuclear fuel cycle benchmarking,
the DTW distance can be used to compare the metric from each simulator to its
Gaussian process model. Using this as an FOM has the advantage of 
incorporating information from the whole time series, rather than just a 
specific point in the cycle.

Many benchmarking studies also wish to create a rank ordering of parameter
importance over all simulators. Examples of such benchmark questions include, 
``In a transition scenario,
which reactor is most important to the total generated power?'' and ``Which
nuclides are most important to the repository heat load?'' DTW distances 
of Gaussian process models of the constient parameters (e.g. the power from
each reactor type) to a Gaussian process model of the total (e.g. total 
generated power) can be used as a FOM itself or to derive other FOMs. 
This paper proposes a novel contribution metric. 
Contribution is taken to be a normalized version of the DTW 
distance for such rank ordering activities.

The remainder of this paper takes a narrative approach that walks through 
a ficticious example benchmarking activity. Generated power [GWe] data 
from DYMOND \cite{yacout2005modeling,feng2015dymond} and Cyclus 
\cite{DBLP:journals/corr/HuffGCFMOSSW15,cyclus_v1_0} is used throughout. 
The underlying fuel cycle being modeled is an
LWR to FR transition scenario that occurs over 200 years, starting from 2010.  
This data should be regarded as for demonstration purposes only. No deep
fuel cycle truths should be directly inferred and the data itself should be 
considered preliminary. However, 
using results from actual fuel cycle simulators shows how
non-judgmental benchmarking works in practice. Completely faked data could 
have been used instead, but this demonstration of the method is more 
believable.

In \S \ref{setup}, the problem is set up, mathematical notation is introduced,
and the raw data from the simulators is presented. In \S \ref{gp}, Gaussian 
process 
modeling is introduced. In \S \ref{dtw}, the dynamic time warping concept is
discussed. In \S \ref{contribution}, the novel contribution metric is 
derived.
\S \ref{filtering} warns against standard time series filtering
techniques. And finally, \S \ref{conclusion} contains concluding remarks
and ideas for future work in fuel cycle benchmarking and for 
using the mechanisms presented here.
