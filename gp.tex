\section{Gaussian Process Modeling}
\label{gp}
The study of Gaussian processes is rich and deep and more completely covered by 
other reseources, such as \cite{rasmussen2006gaussian}. Here only the barest of 
introductions to this topic are given as motivated by the regression problem at
hand. For nuclear fuel cycle benchmark analysis, Gaussian processes will be used
to form a model of the metric time series over all $S$ simulators. 

A Gaussian process is defined by a mean and covariance function. 
The mean function $\mu(t)$ is taken to be the expectation value $\E$ of mean
the input fuctions, which here are the time series for all simulators. The
covariance function $k(t, t^\prime)$ is the expected value of the inputs to the 
mean. Symbolically, 
\begin{equation}
\label{mean-func}
\mu(t) = \E\left[m_s^i(t)\right] = \E\left[m_0^i(t), m_1^i(t), \ldots\right]
\end{equation}
\begin{equation}
\label{covar-func}
k(t, t^\prime) = \E\left[(m_s^i(t) - \mu(t))(m_s^i(t^\prime) - \mu(t^\prime))\right]
\end{equation}
The Gaussian process $\GP$ thus approximates the metric given information from all 
simulators. This is denoted either using functional or operator notation as follows:
\begin{equation}
\label{gp-def-approx}
m^i(t) \approx \GP\left(\mu(t), k(t, t^\prime)\right) \equiv \GP m_s^i
\end{equation}

For Gaussian process regression, a functional form for $k$ needs to be provided.
This representative is sometimes called a kernel function and has free parameters 
for the regression called \emph{hyperparameters}. How hyperparameters are defined is 
tied to the kernel function itself. The values for the hyperparameters are determined
via optimization of the maximal likelihood of the value of the metric function.
While there are many possible kernels, a standard and generically applicable one 
is the exponential squared kernel, as seen in Equation \ref{exp2-kernel}:
\begin{equation}
\label{exp2-kernel}
k(t, t^\prime) = \sigma^2 \exp\left[-\frac{1}{2\ell}(t - t^\prime)^2 \right]
\end{equation}
Here, the length scale $\ell$ and the signal variance $\sigma^2$ are the 
hyperparameters.

Now, define a matrix $K$ such that the element at the $t$-th row and $t^\prime$-th
column is given by Equation \ref{exp2-kernel}. If a vector of training metric 
values $\mathbf{m}$ is defined by contactenating metrics $m_s^i(t)$ for all simulator
for all times $T$, then log likelihood of the obtaining $\mathbf{m}$ 
is seen to be:
\begin{equation}
\label{log-p}
\log p(\mathbf{m}|T) = -\frac{1}{2}\mathbf{m}^\top\left(K + u^2I\right)^{-1}\mathbf{m}
                       -\frac{1}{2}\log\left|K + u^2I\right|
                       -\frac{n}{2}\log 2\pi
\end{equation}
Here, $u$ is the modeling uncertainty as defined in \S\ref{setup}, $I$ is the
identity matrix, and $n$ is the number of training points (the sum of the lengths 
of all of the time series).

Finally, suppose we want to evaluate the Gaussian process regression at a 
series of time points $\mathbf{t_*}$. Let $*$ indicate that the data comes from 
the model, rather than the simulator time series that are used for training. 
The covariance vector between this time grid and the training data is denoted
as $\mathbf{k}_* = \mathbf{k}(\mathbf{t_*})$. The model of the metric function is 
thus seen to be:
\begin{equation}
\label{metric-model}
\mathbf{m}_*(\mathbf{t}_*) = \mathbf{k}_*^\top \left(K + u^2I\right)^{-1}\mathbf{m}
\end{equation}
A full derivation of Equations \ref{mean-func}-\ref{metric-model} can be found in 
\cite{rasmussen2006gaussian}. This resource also contains detailed discussions of 
how to compute the hyperparameters, efficiently invert the covariance matrix, 
and compute the model values. 

In practice, however, a number of free and open source implmenetations of Gaussian 
process regression are readily available. In the Scientific Python ecosystem, both 
scikit-learn v0.17 \cite{scikit-learn} and George v0.2.1 \cite{hodlr} implement 
Gaussian process regression. For the remainder of this paper, George is used
for its superior performance characteristics and easier-to-use interface.


